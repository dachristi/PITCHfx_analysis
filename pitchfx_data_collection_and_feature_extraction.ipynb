{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''pitchfx_data_collection.py\n",
    "\n",
    "David Christiansen\n",
    "1/18/16\n",
    "\n",
    "1. Collect pitcherf/x data from gd2.mlb.com.\n",
    "2. Allocate data for each batter --- write all batter-specific data to a file.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from os import listdir\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web-Scraping and Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the list of URLs for the range of specified dates\n",
    "base_url = 'http://gd2.mlb.com/components/game/mlb/'\n",
    "year = '2014'\n",
    "\n",
    "#GameDay - 404 Not Found\n",
    "# base + year\n",
    "# http://gd2.mlb.com/components/game/mlb/year_2014/ \n",
    "\n",
    "# base + year + month\n",
    "# http://gd2.mlb.com/components/game/mlb/year_2014/month_06/\n",
    "# http://gd2.mlb.com/components/game/mlb/year_2014/month_06/day_11/\n",
    "# http://gd2.mlb.com/components/game/mlb/year_2014/month_06/day_11/gid_2014_06_11_slnmlb_tbamlb_1/\n",
    "# http://gd2.mlb.com/components/game/mlb/year_2014/month_06/day_11/gid_2014_06_11_slnmlb_tbamlb_1/inning/\n",
    "#http://gd2.mlb.com/components/game/mlb/year_2014/month_06/day_11/gid_2014_06_11_slnmlb_tbamlb_1/inning/inning_all.xml\n",
    "\n",
    "r = requests.get(base_url + 'year_' + year)\n",
    "r.text\n",
    "soup1 = BeautifulSoup(r.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "months = [ re.sub(' ', '', t.text) for t in soup1.select('a') if t.text.startswith(' month') ]\n",
    "\n",
    "\n",
    "for month in months:\n",
    "    print month\n",
    "    r = requests.get(base_url + 'year_' + year + '/' + month)\n",
    "    #print r.text\n",
    "    soup2 = BeautifulSoup(r.text, 'html')\n",
    "    days = [ re.sub(' ', '', d.text) for d in soup2.select('a') if d.text.startswith(' day')]\n",
    "    for day in days:\n",
    "        s = requests.get(base_url + 'year_' + year + '/' + month  + day)\n",
    "        soup3 = BeautifulSoup(s.text, 'html')\n",
    "        games = [ re.sub(' ', '', g.text) for g in soup3.select('a') if g.text.startswith(' gid')]\n",
    "        if games != []:\n",
    "            for game in games:\n",
    "                t = requests.get(base_url + 'year_' + year + '/' + month  + day + game + 'inning/inning_all.xml')\n",
    "                #soup4 = BeautifulSoup(t.text, 'xml')\n",
    "                # store the game in a file\n",
    "                #print t.text\n",
    "                with open(\"web_page_data/\" + re.sub('/', '', game) + \".txt\", \"wb\") as file:    \n",
    "                    file.write(t.text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Feature Extraction and Data Alocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature extraction function\n",
    "\n",
    "def feature_extract(pitch):\n",
    "    batter = re.findall('batter=\"(.*?)\"', str(at_bat))[0]\n",
    "    pitcher = re.findall('pitcher=\"(.*?)\"', str(at_bat))[0]\n",
    "    ax = re.findall('ax=\"(.*?)\"', str(pitch))[0]\n",
    "    ay = re.findall('ay=\"(.*?)\"', str(pitch))[0]\n",
    "    az = re.findall('az=\"(.*?)\"', str(pitch))[0]\n",
    "    break_angle = re.findall('break_angle=\"(.*?)\"', str(pitch))[0]\n",
    "    break_length = re.findall('break_length=\"(.*?)\"', str(pitch))[0]\n",
    "    break_y = re.findall('break_y=\"(.*?)\"', str(pitch))[0]\n",
    "    cc = re.findall('cc=\"(.*?)\"', str(pitch))[0]\n",
    "    des = re.findall('des=\"(.*?)\"', str(pitch))[0]\n",
    "    end_speed = re.findall('end_speed=\"(.*?)\"', str(pitch))[0]\n",
    "    #event_num = re.findall('event_num=\"(.*?)\"', str(pitch))[0]\n",
    "    ID = re.findall(' id=\"(.*?)\"', str(pitch))[0]\n",
    "    mt = re.findall('mt=\"(.*?)\"', str(pitch))[0]\n",
    "    nasty = re.findall('nasty=\"(.*?)\"', str(pitch))[0]\n",
    "    pfx_x = re.findall('pfx_x=\"(.*?)\"', str(pitch))[0]\n",
    "    pfx_z = re.findall('pfx_z=\"(.*?)\"', str(pitch))[0]\n",
    "    pitch_type = re.findall('pitch_type=\"(.*?)\"', str(pitch))[0]\n",
    "    #play_guid = re.findall('play_guid=\"(.*?)\"', str(pitch))[0]\n",
    "    px = re.findall('px=\"(.*?)\"', str(pitch))[0]\n",
    "    pz = re.findall('pz=\"(.*?)\"', str(pitch))[0]\n",
    "    spin_dir = re.findall('spin_dir=\"(.*?)\"', str(pitch))[0]\n",
    "    spin_rate = re.findall('spin_rate=\"(.*?)\"', str(pitch))[0]\n",
    "    break_y = re.findall('break_y=\"(.*?)\"', str(pitch))[0]\n",
    "    start_speed = re.findall('start_speed=\"(.*?)\"', str(pitch))[0]\n",
    "    sv_id = re.findall('sv_id=\"(.*?)\"', str(pitch))[0]\n",
    "    sz_bot = re.findall('sz_bot=\"(.*?)\"', str(pitch))[0]\n",
    "    sz_top = re.findall('sz_top=\"(.*?)\"', str(pitch))[0]\n",
    "    tfs = re.findall('tfs=\"(.*?)\"', str(pitch))[0]\n",
    "\n",
    "    tfs_zulu = re.findall('tfs_zulu=\"(.*?)\"', str(pitch))[0]\n",
    "    tfs_zulu = re.sub('T', ' ', tfs_zulu)\n",
    "    tfs_zulu = re.sub('Z', '', tfs_zulu)\n",
    "\n",
    "    Type = re.findall(' type=\"(.*?)\"', str(pitch))[0]\n",
    "    type_confidence = re.findall(' type_confidence=\"(.*?)\"', str(pitch))[0]\n",
    "    vx0 = re.findall(' vx0=\"(.*?)\"', str(pitch))[0]\n",
    "    vy0 = re.findall(' vy0=\"(.*?)\"', str(pitch))[0]\n",
    "    vz0 = re.findall(' vz0=\"(.*?)\"', str(pitch))[0]\n",
    "    x = re.findall(' x=\"(.*?)\"', str(pitch))[0]\n",
    "    x0 = re.findall(' x0=\"(.*?)\"', str(pitch))[0]\n",
    "    y = re.findall(' y=\"(.*?)\"', str(pitch))[0]\n",
    "    y0 = re.findall(' y0=\"(.*?)\"', str(pitch))[0]\n",
    "    z0 = re.findall(' z0=\"(.*?)\"', str(pitch))[0]\n",
    "    zone = re.findall('zone=\"(.*?)\"', str(pitch))[0]\n",
    "\n",
    "    profile = [batter, pitcher, ax, ay, az, break_angle, break_length, break_y,\\\n",
    "               des, end_speed, ID, mt, nasty, pfx_x, pfx_z, pitch_type,\\\n",
    "               px, pz, spin_dir, spin_rate, start_speed, sv_id, sz_bot, sz_top, tfs,\\\n",
    "               tfs_zulu, Type, type_confidence, vx0, vy0, vz0, x, x0, y, y0, z0, zone]\n",
    "    return profile\n",
    "\n",
    "\n",
    "#profile2 = feature_extract(str(pitch))\n",
    "\n",
    "\n",
    "# deleted event_num, play_guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opening_date = datetime.strptime('2014 05 02', '%Y %m %d')\n",
    "files = [ file for file in listdir('web_page_data') if file.endswith('.txt') and \\\n",
    "            datetime.strptime( re.findall('gid_(.{10})', file)[0].replace('_', ' '), '%Y %m %d' ) \\\n",
    "             >= opening_date ]\n",
    "\n",
    "i = 0\n",
    "bad_data = []\n",
    "print len(files)\n",
    "\n",
    "for file in files:\n",
    "    with open( 'web_page_data/'+file, 'rb') as f:\n",
    "        html_content = f.read()\n",
    "    soup = BeautifulSoup(html_content, 'html')\n",
    "    if i%100==0: print i, 'of', len(files)\n",
    "    i += 1\n",
    "    for at_bat in soup.findAll('atbat'):\n",
    "        x = []\n",
    "        batter = re.findall('batter=\"(.+?)\"', str(at_bat))\n",
    "        pitcher = re.findall('pitcher=\"(.+?)\"', str(at_bat))\n",
    "        for pitch in at_bat.findAll('pitch'):\n",
    "            try:\n",
    "                x.append(feature_extract(pitch))\n",
    "            except:\n",
    "                bad_data.append([pitch])\n",
    "                continue\n",
    "        at_bat_data = np.array(x)\n",
    "        with open('batter_data/'+str(batter[0])+'.csv', 'ab') as f:\n",
    "            np.savetxt(f, at_bat_data, delimiter=',', fmt='%s')\n",
    "\n",
    "# started at 4:32pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
